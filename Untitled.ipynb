{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 72)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m72\u001b[0m\n\u001b[0;31m    X = Input(shape=(Tx,human_vocab_size))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)\n",
    "\n",
    "Tx = 30# longest length of dates which is readable for humans\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)#该层接收一个列表的同shape张量，并返回它们的按照给定轴相接构成的向量。\n",
    "#above axis=-1，意思是从倒数第1个维度进行拼接，对于三维矩阵而言，这就等同于axis=2。\n",
    "densor1 = Dense(10, activation=\"tanh\")\n",
    "densor2 = Dense(1, activation=\"relu\")\n",
    "\n",
    "activator = Activation(softmax, name=\"attention_weights\")\n",
    "dotor = Dot(axes=1)\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "\t\"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention \n",
    "    weights \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "\ts_prev = repeator(s_prev)\n",
    "\tconcat = concatenator([a, s_prev])\n",
    "\te = densor1(concat)\n",
    "\tenergies = densor2(e)\n",
    "\talphas = activator(energies)\n",
    "\tcontext = dotor([alphas, a])\n",
    "\n",
    "\treturn context\n",
    "\n",
    "n_a = 32\n",
    "n_s = 64\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)\n",
    "\n",
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab):\n",
    "\t\"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    X = Input(shape=(Tx,human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0') #context vector\n",
    "    s = s0\n",
    "    c = c0\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    #Bidirectional\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
    "    for t in range(Ty):\n",
    "    \tcontext = one_step_attention(a, s)\n",
    "    \t# c is not context but the hidden state of LSTM\n",
    "    \ts, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "    \tout = output_layer(s)\n",
    "    \toutputs.append(out)\n",
    "\n",
    "    model = Model(inputs=[X,s0,c0], outputs=outputs)\n",
    "\n",
    "    return model\n",
    "#define model\n",
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
    "#define optimizer\n",
    "opt = Adam(lr=0.005, beta_1 = 0.9, beta_2 = 0.999, decay = 0.01)\n",
    "#compile\n",
    "model.compile(optimizer = opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#initialization hidden states\n",
    "s0 = np.zeros((m, n_s))# m is the number of training examples\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))\n",
    "# Train\n",
    "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)\n",
    "\n",
    "\n",
    "#####loading big trained model\n",
    "model.load_weights('models/model.h5')\n",
    "\n",
    "#### TEST PART #####\n",
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "for example in EXAMPLES:\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    #change every int in source to one-hot vector\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)), ndmin=3)\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    #print(prediction[0].shape)\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    print(prediction)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))\n",
    "    \n",
    "    #Visualizing Attention\n",
    "    attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab,\n",
    "    \t\"Tuesday 09 Oct 1993\", num = 7, n_s = 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
